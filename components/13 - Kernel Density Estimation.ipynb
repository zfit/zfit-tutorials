{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Kernel Density Estimation\n",
    "\n",
    "This notebook is an introduction into the practical usage of KDEs in zfit and explains the different parameters.\n",
    "*A complete introduction to Kernel Density Estimations, explanations to all methods implemented in zfit and a throughout\n",
    "comparison of the performance can be either found in\n",
    "[Performance of univariate kernel density estimation methods in TensorFlow](https://astroviking.github.io/ba-thesis/)\n",
    "by Marc Steiner from which parts here are taken or in the [documentation of zfit](https://zfit.readthedocs.io/en/latest/)*\n",
    "\n",
    "\n",
    "Kernel Density Estimation is a non-parametric method to estimate the density of a population and offers a more accurate way than a\n",
    "histogram.\n",
    "In a kernel density estimation each data point is substituted with a kernel function\n",
    "that specifies how much it influences its neighboring regions. This kernel functions can then be summed up to get an\n",
    "estimate of the probability density distribution, quite similarly as summing up data points inside bins.\n",
    "\n",
    "However, since\n",
    "the kernel functions are centered on the data points directly, KDE circumvents the problem of arbitrary bin positioning.\n",
    "KDE still depends on the kernel bandwidth (a measure of the spread of the kernel function), however, the total PDF\n",
    "does depend less strongly on the kernel bandwidth than histograms do on bin width and it is much easier to specify\n",
    "rules for an approximately optimal kernel bandwidth than it is to do so for bin width.\n",
    "\n",
    "## Definition\n",
    "\n",
    "Given a set of $n$ sample points $x_k$ ($k = 1,\\cdots,n$), kernel density estimation $\\widehat{f}_h(x)$ is defined as\n",
    "\n",
    "\\begin{equation}\n",
    "\\widehat{f}_h(x) = \\frac{1}{nh} \\sum_{k=1}^n K\\Big(\\frac{x-x_k}{h}\\Big)\n",
    "(\\#eq:kde)\n",
    "\\end{equation}\n",
    "\n",
    "where $K(x)$ is called the kernel function, $h$ is the bandwidth of the kernel and $x$ is the value for which the estimate is calculated. The kernel function defines the shape and size of influence of a single data point over the estimation, whereas the bandwidth defines the range of influence. Most typically a simple Gaussian distribution ($K(x) :=\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}x^2}$) is used as kernel function.\n",
    "The larger the bandwidth parameter $h$ the larger is the range of influence of a single data point on the estimated distribution.\n",
    "\n",
    "## Computational complexity\n",
    "\n",
    "This leads for a large n however to computational problems, as the computational complexity of the exact KDE above is given by $\\mathcal{O}(nm)$ where $n$ is the number of sample points to estimate from and $m$ is the number of evaluation points (the points where you want to calculate the estimate).\n",
    "\n",
    "To circumvent this problem, there exist several approximative methods to decrease this complexity and therefore decrease the runtime as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint on running the notebook**\n",
    "\n",
    "Feel free to rerun a cell a few times. This will change the sample drawn and gives an impression of _how the PDF based on this sample could also look like_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mplhep\n",
    "import numpy as np\n",
    "import zfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact KDE\n",
    "\n",
    "Using the definition above of a KDE, this is implemented in the `KDE1DimExact`. We can start out with a simple Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_wide = zfit.Space('x', (-10, 10))\n",
    "x = np.linspace(-10, 10, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pdf = zfit.pdf.Gauss(obs=obs_wide, mu=0, sigma=2)\n",
    "sample = true_pdf.sample(60)\n",
    "sample_np = zfit.run(sample.value())\n",
    "\n",
    "kde = zfit.pdf.KDE1DimExact(sample,\n",
    "                            # obs,\n",
    "                            # kernel,\n",
    "                            # padding,\n",
    "                            # weights,\n",
    "                            # name\n",
    "                            )\n",
    "plt.plot(x, kde.pdf(x), label='Default exact KDE')\n",
    "plt.plot(x, true_pdf.pdf(x), label='True PDF')\n",
    "plt.plot(sample_np, np.zeros_like(sample_np), 'b|', ms=12)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks already reasonable and we see that the PDF is overestimated in the regions where we sampled, by chance, many events and underestimated in other regions. Since this was a simple example, /et's create a more complitated one (and let's use a bit more samples in order to be able to infer the shape)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss1 = zfit.pdf.Gauss(obs=obs_wide, mu=0, sigma=2)\n",
    "gauss2 = zfit.pdf.Gauss(obs=obs_wide, mu=3, sigma=0.5)\n",
    "true_pdf = zfit.pdf.SumPDF([gauss1, gauss2], fracs=0.85)\n",
    "sample = true_pdf.sample(1000)\n",
    "sample_np = zfit.run(sample.value())\n",
    "\n",
    "kde = zfit.pdf.KDE1DimExact(sample,\n",
    "                            # obs,\n",
    "                            # kernel,\n",
    "                            # padding,\n",
    "                            # weights,\n",
    "                            # name\n",
    "                            )\n",
    "plt.plot(x, kde.pdf(x), label='Default exact KDE')\n",
    "plt.plot(x, true_pdf.pdf(x), label='True PDF')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is more difficult, actually impossible for the current configuration, to approximate the actual PDF well, because we use by default a single bandwidth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bandwidth\n",
    "\n",
    "The bandwidth of a kernel defines it's width, corresponding to the `sigma` of a Gaussian distribution. There is a distinction between global and local bandwidth:\n",
    "\n",
    "<dl>\n",
    "  <dt><strong>Global bandwidth</strong></dt>\n",
    "  <dd>A is a single parameter that is shared amongst all kernels.\n",
    "      While this is a fast and robust method,\n",
    "      it is a rule of thumb approximation. Due to its global nature,\n",
    "      it cannot take into account the different varying\n",
    "      local densities.</dd>\n",
    "  <dt><strong>Local bandwidth</strong></dt>\n",
    "  <dd>A local bandwidth\n",
    "      means that each kernel $i$ has a different bandwidth.\n",
    "      In other words, given some data points with size $n$,\n",
    "      we will need $n$ bandwidth parameters.\n",
    "      This is often more accurate than a global bandwidth,\n",
    "      as it allows to have larger bandwiths in areas of smaller density,\n",
    "      where, due to the small local sample size, we have less certainty\n",
    "      over the true density while having a smaller bandwidth in denser\n",
    "      populated areas.</dd>\n",
    "\n",
    "</dl>\n",
    "\n",
    "We can compare the effects of different global bandwidths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, true_pdf.pdf(x), label='True PDF')\n",
    "\n",
    "for h in [0.1, 0.5, 2, 10]:\n",
    "    kde = zfit.pdf.KDE1DimExact(sample, bandwidth=h)\n",
    "    plt.plot(x, kde.pdf(x), '--', alpha=0.6, label=f'KDE h={h}')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the bandwidth larger makes the KDE less dependent on the randomness of the sample and overfitting of it while it tends to smear out features.\n",
    "0.1 is clearly too wigly while 2 already smears out the feature of having actually two Gaussian peaks.\n",
    "\n",
    "There are a few methods to estimate the optimal global bandwidth, Silvermans (default) and Scotts rule of thumb, respectively. There are also adaptive methods implemented that create an initial density estimation with a rule of thumb and use it to define local bandwidth that are inversely proportional to the (squareroot of) the density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdes_all = [(\"silverman\", '--'), (\"scott\", '--'), (\"adaptive_geom\", ':'), (\"adaptive_zfit\", ':'), (\"isj\", '-.')]\n",
    "kdes_some = [(\"silverman\", '--'), (\"adaptive_zfit\", ':'), (\"isj\", '-.')]\n",
    "for subplot in range(1, 4):\n",
    "    plt.figure(subplot)\n",
    "    if subplot != 1:\n",
    "        # let's zoom in to better see the details\n",
    "        plt.xlim(-2.5, 4)\n",
    "        plt.ylim(0.1, 0.2)\n",
    "    plt.plot(x, true_pdf.pdf(x), 'k', label='True PDF')\n",
    "    kdes = kdes_some if subplot == 3 else kdes_all\n",
    "    for h, fmt in kdes:\n",
    "        kde = zfit.pdf.KDE1DimExact(sample, bandwidth=h)\n",
    "        plt.plot(x, kde.pdf(x), fmt, alpha=0.8, label=f'KDE {h}')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the adaptive method better takes into account the nuances of the peaks. It is very well possible to use local bandwidths directly as an array parameter to bandwidth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel\n",
    "\n",
    "The kernel is the heart of the Kernel Density Estimation, which consists of the sum of\n",
    "kernels around each sample point. Therefore, a kernel should represent\n",
    "the distribution probability of a single data point as close as\n",
    "possible.\n",
    "\n",
    "The most widespread kernel is a Gaussian, or Normal, distribution as many real world example follow it.\n",
    "However, there are many cases where this assumption is not per-se true. In\n",
    "this cases an alternative kernel may offers a better choice.\n",
    "\n",
    "Valid choices are callables that return a\n",
    "[tensorflow_probability.distribution.Distribution](https://www.tensorflow.org/probability/api_docs/python/tfp/distributions?version=nightly), such as all distributions\n",
    "that belong to the [loc-scale family](https://en.wikipedia.org/wiki/Location%E2%80%93scale_family).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, true_pdf.pdf(x), label='True PDF')\n",
    "\n",
    "for kernel in [tfd.Normal, tfd.Cauchy, tfd.Moyal]:\n",
    "    kde = zfit.pdf.KDE1DimExact(sample, kernel=kernel)\n",
    "    plt.plot(x, kde.pdf(x), '--', alpha=0.6, label=f'KDE {kernel.__name__}')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Boundary bias"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}