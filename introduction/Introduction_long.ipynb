{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "%%capture\n",
    "\n",
    "from IPython import get_ipython\n",
    "\n",
    "install_packages = \"google.colab\" in str(get_ipython())\n",
    "if install_packages:\n",
    "    !pip install zfit hepstats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Introduction to zfit\n",
    "\n",
    "In this notebook, we will have a walk through the main components of zfit and their features. Especially the extensive model building part will be discussed separately.\n",
    "zfit consists of 5 mostly independent parts. Other libraries can rely on these parts to do plotting or statistical inference, such as hepstats does. Therefore, we will discuss two libraries in this tutorial: zfit to build models, data and a loss, minimize it and get a fit result and hepstats, to use the loss we built here and do inference.\n",
    "\n",
    "![zfit workflow](../_static/zfit_workflow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data\n",
    "\n",
    "This component in general plays a minor role in zfit: it is mostly to provide a unified interface for data.\n",
    "\n",
    "*Most places do not require the use of zfit data and pandas/array-like or [hist](https://github.com/scikit-hep/hist) objects can be used directly*\n",
    "_(in case of ambiguity, an error is raised)_\n",
    "\n",
    "Preprocessing (apart from a rectangular cut) is therefore not part of zfit and should be done beforehand. Python offers many great possibilities to do so (e.g. Pandas).\n",
    "\n",
    "zfit `Data` can load data from various sources, most notably from Numpy, Pandas DataFrame and ROOT (using uproot) for unbinned data as well as [hist](https://github.com/scikit-hep/hist) histograms for binned data. It is also possible, for convenience, to convert back directly with `to_pandas` or `to_hist`. The _explicity_ constructors are named `from_numpy`, `from_root` etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import zfit\n",
    "# znp is a subset of numpy functions with a numpy-like interface but using actually the zfit backend (currently TF)\n",
    "import zfit.z.numpy as znp\n",
    "import zfit_physics as zphys\n",
    "from zfit import z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "A `Data` needs not only the data itself but also the observables: the human readable string identifiers of the axes (corresponding to \"columns\" of a Pandas DataFrame). It is convenient to define the `Space` not only with the observable but also with a limit: this can directly be re-used as the normalization range in the PDF.\n",
    "\n",
    "First, let's define our observables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "obs = zfit.Space('obs1', -5, 10, label=\"Observable 1\")  # label is optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This `Space` has limits. Next to the effect of handling the observables, we can also play with the limits: multiple `Spaces` can be added to provide disconnected ranges. More importantly, `Space` offers functionality (_currently_, add a `.v1` in front of all these functions for forward compatibility, i.e. `obs.v1.limits`):\n",
    "- limits: the `(lower, upper)` limits as a tuple of 1D arrays\n",
    "- volume: calculate the area/volume (e.g. distance between upper and lower)\n",
    "- labels: Different labels of the spaces\n",
    "- inside(): return a boolean Tensor corresponding to whether the value is _inside_ the `Space`\n",
    "- filter(): filter the input values to only return the one inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "size_normal = 10000\n",
    "data_normal_np = np.random.normal(size=size_normal, scale=2)\n",
    "\n",
    "data_normal = zfit.Data(data=data_normal_np, obs=obs)\n",
    "data_normal = zfit.data.from_numpy(obs=obs, array=data_normal_np)  # or using an explicit constructor with more options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The main functionality is\n",
    "- nevents: attribute that returns the number of events in the object\n",
    "- space: a `Space` that defines the limits of the data; if outside, the data will be cut\n",
    "- n_obs: defines the number of dimensions in the dataset\n",
    "- with_obs: returns a subset of the dataset with only the given obs\n",
    "- weights: event based weights\n",
    "\n",
    "Furthermore, `value` returns a Tensor with shape `(nevents, n_obs)`.\n",
    "\n",
    "To retrieve values, in general `z.unstack_x(data)` should be used; this returns a single Tensor with shape (nevents) or a list of tensors if `n_obs` is larger then 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"We have {data_normal.nevents} events in our dataset with the minimum of {np.min(data_normal.unstack_x())}\")  # remember! The obs cut out some of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_normal.n_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model\n",
    "\n",
    "Building models is by far the largest part of zfit. We will therefore cover an essential part, the possibility to build custom models, in an extra chapter. Let's start out with the idea that you define your parameters and your observable space; the latter is the expected input data.\n",
    "\n",
    "There are two types of models in zfit:\n",
    "- functions, which are rather simple and \"underdeveloped\"; their usage is often not required.\n",
    "- PDF that are function which are normalized (over a specified range); this is the main model and is what we gonna use throughout the tutorials.\n",
    "\n",
    "A PDF is defined by\n",
    "\n",
    "\\begin{align}\n",
    "\\mathrm{PDF}_{f(x)}(x; \\theta) = \\frac{f(x; \\theta)}{\\int_{a}^{b} f(x; \\theta)}\n",
    "\\end{align}\n",
    "\n",
    "where a and b define the normalization range (`norm`), over which (by inserting into the above definition) the integral of the PDF is unity.\n",
    "\n",
    "zfit has a modular approach to things and this is also true for models. While the normalization itself (e.g. what are parameters, what is normalized data) will already be pre-defined in the model, models are composed of functions that are transparently called inside. For example, a Gaussian would usually be implemented by writing a Python function `def gauss(x, mu, sigma)`, which does not care about the normalization and then be wrapped in a PDF, where the normalization and what is a parameter is defined.\n",
    "\n",
    "In principle, we can go far by using simply functions (e.g. [TensorFlowAnalysis/AmpliTF](https://github.com/apoluekt/AmpliTF) by Anton Poluektov uses this approach quite successfully for Amplitude Analysis), but this design has limitations for a more general fitting library such as zfit (or even [TensorWaves](https://github.com/ComPWA/tensorwaves), being built on top of AmpliTF).\n",
    "The main thing is to keep track of the different ordering of the data and parameters, especially the dependencies.\n",
    "\n",
    "\n",
    "Let's create a simple Gaussian PDF. We already defined the `Space` for the data before, now we only need the parameters. This are a different object than a `Space`.\n",
    "\n",
    "### Parameter\n",
    "A `Parameter` (there are different kinds actually, more on that later) takes the following arguments as input:\n",
    "`Parameter(human readable name, initial value[, lower limit, upper limit])` where the limits are recommended but not mandatory. Furthermore, `step_size` can be given (which is useful to be around the given uncertainty, e.g. for large yields or small values it can help a lot to set this). Also, a `floating` argument is supported, indicating whether the parameter is allowed to float in the fit or not (just omitting the limits does _not_ make a parameter constant).\n",
    "\n",
    "The name of the parameter identifies it; therefore, while multiple parameters with the same name can exist, they cannot exist inside the same model/loss/function, as they would be ambiguous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mu = zfit.Parameter('mu', 1, -3, 3, step_size=0.2)\n",
    "another_mu = zfit.Parameter('mu', 2, -3, 3, step_size=0.2)\n",
    "sigma_num = zfit.Parameter('sigma42', 1, 0.1, 10, floating=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "These attributes can be changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"sigma is float: {sigma_num.floating}\")\n",
    "sigma_num.floating = True\n",
    "print(f\"sigma is float: {sigma_num.floating}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we have everything to create a Gaussian PDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gauss = zfit.pdf.Gauss(obs=obs, mu=mu, sigma=sigma_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Since this holds all the parameters and the observables are well defined, we can retrieve them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gauss.n_obs  # dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gauss.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gauss.space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gauss.norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As we've seen, the `obs` we defined is the `space` of Gauss: this acts as the default limits whenever needed (e.g. for sampling). `gauss` also has a `norm`, which equals by default as well to the `obs` given, however, we can explicitly change that with `set_norm`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can also access the parameters of the PDF in two ways, depending on our intention:\n",
    "either by _name_ (the parameterization name, e.g. `mu` and `sigma`, as defined in the `Gauss`), which is useful if we are interested in the parameter that _describes_ the shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gauss.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "or to retrieve all the parameters that the PDF depends on. As this now may sounds trivial, we will see later that models can depend on other models (e.g. sums) and parameters on other parameters. There is one function that automatically retrieves _all_ dependencies, `get_params`. It takes three arguments to filter:\n",
    "- floating: whether to filter only floating parameters, only non-floating or don't discriminate\n",
    "- is_yield: if it is a yield, or not a yield, or both\n",
    "- extract_independent: whether to recursively collect all parameters. This, and the explanation for why independent, can be found later on in the `Simultaneous` tutorial.\n",
    "\n",
    "Usually, the default is exactly what we want if we look for _all free parameters that this PDF depends on_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gauss.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The difference will also be clear if we e.g. use the same parameter twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gauss_only_mu = zfit.pdf.Gauss(obs=obs, mu=mu, sigma=mu)\n",
    "print(f\"params={gauss_only_mu.params}\")\n",
    "print(f\"get_params={gauss_only_mu.get_params()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Functionality\n",
    "\n",
    "PDFs provide a few useful methods. The main features of a zfit PDF are:\n",
    "\n",
    "- `pdf`: the normalized value of the PDF. It takes an argument `norm` that can be set to `False`, in which case we retrieve the unnormalized value\n",
    "- `integrate`: given a certain range, the PDF is integrated. As `pdf`, it takes a `norm` argument that integrates over the unnormalized `pdf` if set to `False`\n",
    "- `sample`: samples from the pdf and returns a `Data` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "integral = gauss.integrate(limits=(-1, 3))  # corresponds to 2 sigma integral\n",
    "integral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Tensors\n",
    "\n",
    "As we see, many zfit functions return Tensors. This is however no magical thing! If we're outside of models, than we can always safely convert them to a numpy array by calling `zfit.run(...)` on it (or any structure containing potentially multiple Tensors). However, this may not even be required often! They can be added just like numpy arrays and interact well with Python and Numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.sqrt(integral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "They also have shapes, dtypes, can be slices etc. So do not convert them except you need it. More on this can be seen in the talk later on about zfit and TensorFlow 2.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sample = gauss.sample(n=1000)  # default space taken as limits\n",
    "print(f\"As DataFrame: {sample.to_pandas()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"To array-like: {sample.value()[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(sample['obs1'])[:10]  # index like a pandas DataFrame, use like an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sample.to_numpy().shape  # _explicitly_ to numpy (usually not needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sample.nevents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sample.obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We see that sample returns also a zfit `Data` object with the same space as it was sampled in. This can directly be used e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "probs = gauss.pdf(sample)\n",
    "probs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**NOTE**: In case you want to do this repeatedly (e.g. for toy studies), there is a way more efficient way (see later on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Plotting\n",
    "\n",
    "so far, we have a dataset and a PDF. Before we go for fitting, we can make a plot. This functionality is not _extensively_ covered in zfit, but helper functions exist. It is however simple enough to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_model(model, data, scale=1, plot_data=True):  # we will use scale later on\n",
    "\n",
    "    nbins = 50\n",
    "\n",
    "    lower, upper = data.space.v1.limits\n",
    "    x = znp.linspace(lower, upper, num=1000)  # np.linspace also works\n",
    "    y = model.pdf(x) * size_normal / nbins * data.space.volume\n",
    "    y *= scale\n",
    "    plt.plot(x, y)\n",
    "    data_plot = zfit.run(z.unstack_x(data))  # we could also use the `to_pandas` method\n",
    "    if plot_data:\n",
    "        plt.hist(data_plot, bins=nbins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_model(gauss, data_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shortcut to plot pdf\n",
    "gauss.plot.plotpdf(full=True)  # full plots all the labels, legends as well. False means plot only PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can of course do better (and will see that later on, continuously improve the plots), but this is quite simple and gives us the full power of matplotlib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Different models\n",
    "\n",
    "zfit offers a selection of predefined models (and extends with models from zfit-physics that contain physics specific models such as ARGUS shaped models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(zfit.pdf.__all__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(zphys.pdf.__all__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To create a more realistic model, we can build some components for a mass fit with a\n",
    "- signal component: CrystalBall\n",
    "- combinatorial background: Exponential\n",
    "- partial reconstructed background on the left: Kernel Density Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mass_obs = zfit.Space('mass', 0, 1000, label=r\"$m(\\mu\\mu) [MeV/c^2]$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Signal component\n",
    "\n",
    "mu_sig = zfit.Parameter('mu_sig', 400, 100, 600)\n",
    "sigma_sig = zfit.Parameter('sigma_sig', 50, 1, 100)\n",
    "alpha_sig = zfit.Parameter('alpha_sig', 200, 100, 400, floating=False)  # won't be used in the fit\n",
    "n_sig = zfit.Parameter('n sig', 4, 0.1, 30, floating=False)\n",
    "signal = zfit.pdf.CrystalBall(obs=mass_obs, mu=mu_sig, sigma=sigma_sig, alpha=alpha_sig, n=n_sig)\n",
    "ax = signal.plot.plotpdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# combinatorial background\n",
    "\n",
    "lam = zfit.Parameter('lambda', -0.01, -0.05, -0.00001)\n",
    "comb_bkg = zfit.pdf.Exponential(lam, obs=mass_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "part_reco_data = np.random.normal(loc=200, scale=150, size=20_000)\n",
    "part_reco_data = zfit.Data.from_numpy(obs=mass_obs,\n",
    "                                      array=part_reco_data)  # we don't need to do this but now we're sure it's inside the limits\n",
    "\n",
    "part_reco = zfit.pdf.KDE1DimISJ(obs=mass_obs, data=part_reco_data)\n",
    "part_reco.plot.plotpdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Composing models\n",
    "\n",
    "We can also compose multiple models together. Here we'll stick to one dimensional models, the extension to multiple dimensions is explained in the \"custom models tutorial\".\n",
    "\n",
    "Here we will use a `SumPDF`. This takes pdfs and fractions. If we provide n pdfs and:\n",
    "- n - 1 fracs: the nth fraction will be 1 - sum(fracs)\n",
    "- n fracs: no normalization attempt is done by `SumPDf`. If the fracs are not implicitly normalized, this can lead to bad fitting\n",
    "  behavior if there is a degree of freedom too much\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sig_frac = zfit.Parameter('sig_frac', 0.3, 0, 1)\n",
    "comb_bkg_frac = zfit.Parameter('comb_bkg_frac', 0.25, 0, 1)\n",
    "model = zfit.pdf.SumPDF([signal, comb_bkg, part_reco], [sig_frac, comb_bkg_frac])\n",
    "ax = model.plot.plotpdf(full=True)  # just a quick way of plotting the pdf\n",
    "model.plot.comp.plotpdf(ax=ax, linestyle='--')  # plot the components, only works for pdfs with components to plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In order to have a corresponding data sample, we can just create one. Since we want to fit to this dataset later on, we will create it with slightly different values. Therefore, we can use the ability of a parameter to be set temporarily to a certain value with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"before: {sig_frac}\")\n",
    "with sig_frac.set_value(0.25):\n",
    "    print(f\"new value: {sig_frac}\")\n",
    "print(f\"after 'with': {sig_frac}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "While this is useful, it does not fully scale up. We can use the `zfit.param.set_values` helper therefore.\n",
    "(_Sidenote: instead of a list of values, we can also use a `FitResult`, the given parameters then take the value from the result_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with zfit.param.set_values([mu_sig, sigma_sig, sig_frac, comb_bkg_frac, lam], [370, 34, 0.18, 0.15, -0.006]):\n",
    "    data = model.sample(n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# even better, we directly give the values we want\n",
    "data = model.sample(n=10000, params={mu_sig: 370, sigma_sig: 34, sig_frac: 0.18, comb_bkg_frac: 0.15, lam: -0.006})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_model(model, data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Plotting the components is not difficult now: we can either just plot the pdfs separately (as we still can access them) or in a generalized manner by accessing the `pdfs` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_comp_model(model, data):\n",
    "    for mod, frac in zip(model.pdfs, model.params.values()):\n",
    "        plot_model(mod, data, scale=frac, plot_data=False)\n",
    "    plot_model(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_comp_model(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we can add legends etc. Btw, did you notice that actually, the `frac` params are zfit `Parameters`? But we just used them as if they were Python scalars and it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(model.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Extended PDFs\n",
    "\n",
    "So far, we have only looked at normalized PDFs that do contain information about the shape but not about the _absolute_ scale. We can make a PDF extended by adding a yield to it.\n",
    "\n",
    "The behavior of the new, extended PDF does **NOT change**, any methods we called before will act the same. Only exception, some may require an argument _less_ now. All the methods we used so far will return the same values. What changes is that the flag `model.is_extended` now returns `True`. Furthermore, we have now a few more methods that we can use which would have raised an error before:\n",
    "- `get_yield`: return the yield parameter (notice that the yield is _not_ added to the shape parameters `params`)\n",
    "- `ext_{pdf,integrate}`: these methods return the same as the versions used before, however, multiplied by the yield\n",
    "- `sample` is still the same, but does not _require_ the argument `n` anymore. By default, this will now equal to a _poissonian sampled_ n around the yield.\n",
    "\n",
    "The `SumPDF` now does not strictly need `fracs` anymore: if _all_ input PDFs are extended, the sum will be as well and use the (normalized) yields as fracs\n",
    "\n",
    "The preferred way to create an extended PDf is to use `PDF.create_extended(yield)`. However, since this relies on copying the PDF (which may does not work for different reasons), there is also a `set_yield(yield)` method that sets the yield in-place. This won't lead to ambiguities, as everything is supposed to work the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "yield_model = zfit.Parameter('yield_model', 10000, 0, 20000, step_size=10)\n",
    "model_ext = model.create_extended(yield_model)\n",
    "\n",
    "# or create directly\n",
    "model_ext = zfit.pdf.SumPDF([signal, comb_bkg, part_reco], fracs=[sig_frac, comb_bkg_frac], extended=yield_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "More common is to create the models as extended and sum them up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sig_yield = zfit.Parameter('sig_yield', 2000, 0, 10000, step_size=1)\n",
    "sig_ext = zfit.pdf.Gauss(obs=mass_obs, mu=mu_sig, sigma=sigma_sig, extended=sig_yield)\n",
    "\n",
    "comb_bkg_yield = zfit.Parameter('comb_bkg_yield', 6000, 0, 10000, step_size=1)\n",
    "comb_bkg_ext = comb_bkg.create_extended(comb_bkg_yield)  # or using an existing model\n",
    "\n",
    "part_reco_yield = zfit.Parameter('part_reco_yield', 2000, 0, 10000, step_size=1)\n",
    "part_reco_ext = zfit.pdf.KDE1DimFFT(obs=mass_obs, data=part_reco_data, extended=part_reco_yield)  # another KDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_ext_sum = zfit.pdf.SumPDF([sig_ext, comb_bkg_ext, part_reco_ext])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Loss\n",
    "\n",
    "A loss combines the model and the data, for example to build a likelihood. Furthermore, it can contain constraints, additions to the likelihood. Currently, if the `Data` has weights, these are automatically taken into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nll_gauss = zfit.loss.UnbinnedNLL(gauss, data_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The loss has several attributes to be transparent to higher level libraries. We can calculate the value of it using `value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nll_gauss.value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Notice that due to graph building, this will take significantly longer on the first run. Rerun the cell above and it will be way faster.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Furthermore, the loss also provides a possibility to calculate the gradients or, often used, the value and the gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can access the data and models (and possible constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nll_gauss.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nll_gauss.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Similar to the models, we can also get the parameters via `get_params`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nll_gauss.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Extended loss\n",
    "\n",
    "More interestingly, we can now build a loss for our composite sum model using the sampled data. Since we created an extended model, we can now also create an extended likelihood, taking into account a Poisson term to match the yield to the number of events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nll = zfit.loss.ExtendedUnbinnedNLL(model_ext_sum, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nll.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Minimization\n",
    "\n",
    "While a loss is interesting, we usually want to minimize it. Therefore, we can use the minimizers in zfit, most notably `Minuit`, a wrapper around the [iminuit minimizer](https://github.com/scikit-hep/iminuit).\n",
    "\n",
    "\n",
    "Given that iminuit provides us with a very reliable and stable minimizer, it is usually recommended to use this. Others are implemented as well and could easily be wrapped, however, the convergence rate can vary.\n",
    "\n",
    "All the minimizers have the same options, if available, and use by default the _same criteria for convergence (EDM, like iminuit).\n",
    "- `tol`: the Estimated Distance to Minimum (EDM) criteria for convergence (default 1e-3)\n",
    "- `verbosity`: between 0 and 10, 5 is normal, 7 is verbose, 10 is maximum\n",
    "- `gradient`: if True, uses the Minuit numerical gradient instead of the TensorFlow gradient. This is usually more stable for smaller fits; furthermore the TensorFlow gradient _can_ (experience based) sometimes be wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "minimizer = zfit.minimize.Minuit()\n",
    "# minimizer = zfit.minimize.NLoptLBFGSV1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For the minimization, we can call `minimize`, which takes a\n",
    "- loss as we created above\n",
    "- (optional) the parameters to minimize. Default to `loss.get_params()`, i.e. all free parameters\n",
    "- (optional) a previous, similar result. This can be used to help with the minimization by providing information about the minimum, uncertainties etc.\n",
    "\n",
    "By default, `minimize` uses all the free floating parameters (obtained with `get_params`). We can also explicitly specify which ones to use by giving them (or better, objects that depend on them) to `minimize`; note however that non-floating parameters, even if given explicitly to `minimize` won 't be minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Pre-fit parts of the PDF\n",
    "\n",
    "Before we want to fit the whole PDF however, it can be useful to pre-fit it. A way can be to fix the combinatorial background by fitting the exponential to the right tail.\n",
    "\n",
    "Therefore we create a new data object with an additional cut and furthermore, set the normalization range of the background pdf to the range we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "values = z.unstack_x(data)\n",
    "obs_right_tail = zfit.Space('mass', (550, 1000))\n",
    "data_tail = zfit.Data(obs=obs_right_tail, data=values)\n",
    "comb_bkg_right = comb_bkg.to_truncated(limits=obs_right_tail, norm=obs_right_tail)  # this gets the normalization right\n",
    "nll_tail = zfit.loss.UnbinnedNLL(comb_bkg_right, data_tail)\n",
    "result_sideband = minimizer.minimize(nll_tail)\n",
    "print(result_sideband)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Since we now fit the lambda parameter of the exponential, we can fix it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lam.floating = False\n",
    "lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "result = minimizer.minimize(nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_comp_model(model_ext_sum, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Fit result\n",
    "\n",
    "The result of every minimization is stored in a `FitResult`. This is the last stage of the zfit workflow and serves as the interface to other libraries. Its main purpose is to store the values of the fit, to reference to the objects that have been used and to perform (simple) uncertainty estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This gives an overview over the whole result. Often we're mostly interested in the parameters and their values, which we can access with a `params` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(result.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This is a `dict` which stores any knowledge about the parameters and can be accessed by the parameter (object) itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "result.params[mu_sig]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "'value' is the value at the minimum. To obtain other information about the minimization process, `result` contains more attributes:\n",
    "- fmin: the function minimum\n",
    "- edm: estimated distance to minimum\n",
    "- info: contains a lot of information, especially the original information returned by a specific minimizer\n",
    "- converged: if the fit converged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "result.fmin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Estimating uncertainties\n",
    "\n",
    "The `FitResult` has mainly two methods to estimate the uncertainty:\n",
    "- a profile likelihood method (like MINOS)\n",
    "- Hessian approximation of the likelihood (like HESSE)\n",
    "\n",
    "When using `Minuit`, this uses (currently) it's own implementation. However, zfit has its own implementation, which are likely to become the standard and can be invoked by changing the method name.\n",
    "\n",
    "Hesse is also [on the way to implement](https://github.com/zfit/zfit/pull/244) the [corrections for weights](https://inspirehep.net/literature/1762842).\n",
    "\n",
    "We can explicitly specify which parameters to calculate, by default it does for all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "result.hesse(method='minuit_hesse', name='hesse')  # these are the default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# result.hesse(method='hesse_np', name='hesse_np')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We get the result directly returned. This is also added to `result.params` for each parameter and is nicely displayed with an added column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "errors, new_result = result.errors(params=[sig_yield, part_reco_yield, mu_sig],\n",
    "                                   name='errors')  # just using three for speed reasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# errors, new_result = result.errors(params=[yield_model, sig_frac, mu_sig], method='zfit_errors', name='error with zfit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### What is 'new_result'?\n",
    "\n",
    "When profiling a likelihood, such as done in the algorithm used in `errors`, a new minimum can be found. If this is the case, this new minimum will be returned, otherwise `new_result` is `None`. Furthermore, the current `result` would be rendered invalid by setting the flag `valid` to `False`. _Note_: this behavior only applies to the zfit internal error estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### A simple profile\n",
    "\n",
    "There is no default function (yet) for simple profiling plot. However, again, we're in Python and it's simple enough to do that for a parameter. Let's do it for `sig_yield`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(1600, 1850, num=50)\n",
    "y = []\n",
    "sig_yield.floating = False\n",
    "tmpres = None\n",
    "for val in tqdm.tqdm(x):\n",
    "    sig_yield.set_value(val)\n",
    "    tmpres = minimizer.minimize(nll, init=tmpres)\n",
    "    y.append(nll.value())\n",
    "\n",
    "sig_yield.floating = True\n",
    "result.update_params();  # reset params, equivalent to below\n",
    "# zfit.param.set_values(nll.get_params(), result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can also access the covariance matrix of the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "result.covariance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and loading\n",
    "\n",
    "A `FitResult` can be saved to disk and loaded back, as can be any other zfit object. This is done using `dill`, which is like `pickle`, a Python standard library to serialize arbitrary Python objects.\n",
    "This allows not only to store single objects but any combination of them. It is recommended therfore to store a `dict`, or anything arbitrary, with all the objects that are required to reproduce the result.\n",
    "(the `FitResult` itself stores all objects too, so it is usually enough to store the `FitResult` itself)\n",
    "\n",
    "Make sure to dumb and load *all required objects* at once. Otherwise, there will be issues regarding parameter names, as it is not clear whether the same or a different object is meant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumped_res = zfit.dill.dumps(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumbed_ws = zfit.dill.dumps({'result': result, 'model': model_ext_sum, 'cov': result.covariance(), # usually not needed, as result contains all information \n",
    "                             'other': ['any', 'other', 'object', mu], 'profile': y})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_res = zfit.dill.loads(dumped_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try to dump and load losses or PDFs in a HS$^3$-like format, a human-readable serialization format. This is however not recommended for long-term storage, as the format may change and is not guaranteed to be stable. (but great for publishing the likelihood!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zfit.hs3.dumps(nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or just a PDF/parts of it\n",
    "signal.to_yaml()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### End of zfit\n",
    "\n",
    "This is where zfit finishes and other libraries take over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Beginning of hepstats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "`hepstats` is a library containing statistical tools and utilities for high energy physics. In particular you do statistical inferences using the models and likelhoods function constructed in `zfit`.\n",
    "\n",
    "Short example: let's compute for instance a confidence interval at 68 % confidence level on the mean of the gaussian defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from hepstats.hypotests import ConfidenceInterval\n",
    "from hepstats.hypotests.calculators import AsymptoticCalculator\n",
    "from hepstats.hypotests.parameters import POIarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "calculator = AsymptoticCalculator(input=result, minimizer=minimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "value = result.params[mu_sig][\"value\"]\n",
    "error = result.params[mu_sig][\"hesse\"][\"error\"]\n",
    "\n",
    "mean_scan = POIarray(mu_sig, np.linspace(value - 1.5 * error, value + 1.5 * error, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ci = ConfidenceInterval(calculator, mean_scan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ci.interval()  # fix in hepstats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from utils import one_minus_cl_plot\n",
    "\n",
    "# ax = one_minus_cl_plot(ci)\n",
    "# ax.set_xlabel(\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "There will be more of `hepstats` later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
