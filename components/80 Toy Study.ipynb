{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy studies\n",
    "\n",
    "Having a model, it can be convenient to do sensitivity studies and checks of the fit by doing a \"toy study\": sampling from the model and fitting to the generated sample. The fitted values and the spread characterize whether the fit is biased or not. The difference to the \"actual\" value divided by the uncertainty (the pulls) should follow a standard Gaussian distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import progressbar\n",
    "import tensorflow as tf\n",
    "import zfit\n",
    "from zfit import z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gonna build a simple model, just a Gaussian. But, given the well defined workflow of zfit, `model` can be exchanged by _any_ complicated composition or custom model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = zfit.Space('x', (-5, 5))\n",
    "\n",
    "sigma = zfit.Parameter('sigma', 1, 0.1, 10)\n",
    "mu = zfit.Parameter('mu', 0, -1, 1)\n",
    "model = zfit.pdf.Gauss(obs=obs, mu=mu, sigma=sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using `sample` as before, we will first build our loss with a more efficient `Data`, a \"sampler\", created by `create_sampler`. This has like `sample` the arguments for limits and the number of samples, but also supports `fixed_params`, which is true by default. This means that whenever this object is _resampled_, it will be resampled with the parameter values that it had when we created the sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = model.create_sampler(n=3000, fixed_params=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, no sampling happened yet. But first, we build our whole chain, just using our sampler as `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nll = zfit.loss.UnbinnedNLL(model, sampler)\n",
    "\n",
    "from zfit.minimize import \\\n",
    "    DefaultToyStrategy  # this stategy does not raise an error with NaNs but returns a non-converged `FitResult`\n",
    "\n",
    "minimizer = zfit.minimize.Minuit(strategy=DefaultToyStrategy(), verbosity=0, tol=1e-3, use_minuit_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_results = []\n",
    "ntoys = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = nll.get_params()\n",
    "\n",
    "with progressbar.ProgressBar(max_value=ntoys) as bar:\n",
    "\n",
    "    while len(fit_results) < ntoys:\n",
    "\n",
    "        # Generate toys\n",
    "        sampler.resample()  # this is where the sampling happens\n",
    "\n",
    "        # Randomise initial values. They can put the pdf in an unphysical region, making it negative at points.\n",
    "        # This will produce NaNs in the log of the NLL. Therefore, we randomize until we got no NaNs anymore.\n",
    "        for param in params:\n",
    "            param.randomize()  # or smarter, use `set_value` for your own method\n",
    "\n",
    "# The following can be used if the loss may returns NaNs, to test. Repeat in a while loop until it matches\n",
    "#            try:\n",
    "#                is_nan = np.isnan(zfit.run(nll.value()))\n",
    "#            except tf.errors.InvalidArgumentError:  # NaNs produced, check_numerics raises this error\n",
    "#                # print(\"nan error, try again\")  # try again\n",
    "#                is_nan = True\n",
    "#            else:\n",
    "#                break\n",
    "\n",
    "        # Minimise the NLL\n",
    "        result = minimizer.minimize(nll)\n",
    "\n",
    "        if result.converged:\n",
    "            # Calculate uncertainties\n",
    "            result.hesse()\n",
    "            fit_results.append(result)\n",
    "            bar.update(len(fit_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fit_results[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate results\n",
    "\n",
    "From here on, we can use the fit_results to compare against the true value, make plots, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
